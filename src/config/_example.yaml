meta:
    name: 'baseline_mbv3s_vww96'
    seed: 42
    out_dir: 'runs'
    device: 'auto' # auto|cuda|mps|cpu

data:
    path: 'data/vww96'
    batch: 256
    num_workers: 4
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    aug:
        rand_hflip: 0.5
        color_jitter: [0.2, 0.2, 0.2, 0.0] # [b, c, s, h]
        random_erasing: [0.25, 0.02, 0.12, 0.3, 3.3] # [p, scale-, scale+, ratio-, ratio+]

model:
    type: 'mobilenet_v3_small' # mobilenet_v3_small | mobilenet_v3_large
    pretrained: false
    # dropout: 0.0 ?

train:
    epochs: 30
    label_smoothing: 0.05
    early_stop_patience: 5
    grad_clip_norm: 2.0
    optimizer:
        name: 'adamw'
        lr: 0.001
        weight_decay: 0.0001
    scheduler:
        name: 'onecycle'
        max_lr: 0.001
        pct_start: 0.05
        div_factor: 20.0
        final_div_factor: 10000.0

# Only used when stage=kd
kd:
    teacher:
        checkpt: 'runs\2025-10-28_22-11-40_teacher_effnetb2_vww96\model.pt'
        arch: 'efficientnet_b2'
        pretrained: true
    alpha: 0.55
    alpha_constant: true
    temperature: 2.0
    teacher_input_size: 96
    label_smoothing: 0.01
    confidence_gamma: 2.0
    margin_weight: 0.03
    margin_weight_start:
    margin_weight_end:
    margin_weight_decay_end_epoch:

# Only used when stage=prune
prune:
    strategy: 'global'
    importance: 'bn_gamma' # "bn_gamma" or "l1_norm"
    expand_only: true
    targets: [0.10, 0.20, 0.30]
    protect:
        block_min_channels: 16
        skip_first_n: 1
        keep_block_indices: []
    finetune:
        use_kd: true
        epochs: 4
        lr: 0.0001
        lr_high: 0.00005
        lr_high_threshold: 0.25
        weight_decay: 0.00001
        patience: 2
        bn_recalibrate_loader: val
        bn_recalibrate_batches: 32
    grad_clip_norm: 2.0
    acceptance:
        baseline_accuracy: 0.8562
        max_drop_after_step: 0.015

# Only used when stage=quant
quant:
    mode: 'ptq' # ptq | qat
    calibrate_split: 'val' # 'train' or 'val'
    calibrate_batches: 512
    ptq:
        exclude_first_last: true
    qat:
        epochs: 5
        lr: 0.00005
        weight_decay: 0.00001
        patience: 2
        grad_clip_norm: 2.0
        exclude_first_last: false
    kd:
        use_kd: true
        alpha: 0.55 # override optional, fallback to top-level kd.* if omitted
        temperature: 3.0
        margin_weight: 0.0
        label_smoothing: 0.0
